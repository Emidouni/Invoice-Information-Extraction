{
  "best_metric": 0.8270833333333333,
  "best_model_checkpoint": "swin-tiny-patch4-window7-224-finetuned-eurosat/checkpoint-3000",
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 5.237224102020264,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 2.767,
      "step": 10
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 6.001794338226318,
      "learning_rate": 6.666666666666667e-05,
      "loss": 2.7037,
      "step": 20
    },
    {
      "epoch": 0.1,
      "grad_norm": 19.913122177124023,
      "learning_rate": 0.0001,
      "loss": 2.5238,
      "step": 30
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 24.669700622558594,
      "learning_rate": 0.00013333333333333334,
      "loss": 2.2101,
      "step": 40
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 13.706263542175293,
      "learning_rate": 0.00016666666666666666,
      "loss": 2.0413,
      "step": 50
    },
    {
      "epoch": 0.2,
      "grad_norm": 13.29438304901123,
      "learning_rate": 0.0002,
      "loss": 1.9029,
      "step": 60
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 13.300641059875488,
      "learning_rate": 0.00023333333333333333,
      "loss": 1.8692,
      "step": 70
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 11.581512451171875,
      "learning_rate": 0.0002666666666666667,
      "loss": 1.6572,
      "step": 80
    },
    {
      "epoch": 0.3,
      "grad_norm": 24.0253963470459,
      "learning_rate": 0.0003,
      "loss": 1.6802,
      "step": 90
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 13.486605644226074,
      "learning_rate": 0.0003333333333333333,
      "loss": 1.7485,
      "step": 100
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 19.24017906188965,
      "learning_rate": 0.00036666666666666667,
      "loss": 1.6054,
      "step": 110
    },
    {
      "epoch": 0.4,
      "grad_norm": 15.232032775878906,
      "learning_rate": 0.0004,
      "loss": 1.5231,
      "step": 120
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 8.963571548461914,
      "learning_rate": 0.00043333333333333337,
      "loss": 1.589,
      "step": 130
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 9.150212287902832,
      "learning_rate": 0.00046666666666666666,
      "loss": 1.4898,
      "step": 140
    },
    {
      "epoch": 0.5,
      "grad_norm": 10.451428413391113,
      "learning_rate": 0.0005,
      "loss": 1.5525,
      "step": 150
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 11.30177116394043,
      "learning_rate": 0.0005333333333333334,
      "loss": 1.5957,
      "step": 160
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 14.441909790039062,
      "learning_rate": 0.0005666666666666667,
      "loss": 1.4638,
      "step": 170
    },
    {
      "epoch": 0.6,
      "grad_norm": 15.00700855255127,
      "learning_rate": 0.0006,
      "loss": 1.5245,
      "step": 180
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 16.772401809692383,
      "learning_rate": 0.0006333333333333333,
      "loss": 1.5684,
      "step": 190
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 9.098340034484863,
      "learning_rate": 0.0006666666666666666,
      "loss": 1.5084,
      "step": 200
    },
    {
      "epoch": 0.7,
      "grad_norm": 12.226120948791504,
      "learning_rate": 0.0007,
      "loss": 1.4703,
      "step": 210
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 7.3179192543029785,
      "learning_rate": 0.0007333333333333333,
      "loss": 1.3546,
      "step": 220
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 10.043662071228027,
      "learning_rate": 0.0007666666666666667,
      "loss": 1.5092,
      "step": 230
    },
    {
      "epoch": 0.8,
      "grad_norm": 10.910467147827148,
      "learning_rate": 0.0008,
      "loss": 1.4782,
      "step": 240
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 11.483963966369629,
      "learning_rate": 0.0008333333333333334,
      "loss": 1.4649,
      "step": 250
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 9.579549789428711,
      "learning_rate": 0.0008666666666666667,
      "loss": 1.4509,
      "step": 260
    },
    {
      "epoch": 0.9,
      "grad_norm": 10.774528503417969,
      "learning_rate": 0.0009000000000000001,
      "loss": 1.491,
      "step": 270
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 7.986006736755371,
      "learning_rate": 0.0009333333333333333,
      "loss": 1.512,
      "step": 280
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 8.93535041809082,
      "learning_rate": 0.0009666666666666667,
      "loss": 1.5233,
      "step": 290
    },
    {
      "epoch": 1.0,
      "grad_norm": 17.242706298828125,
      "learning_rate": 0.001,
      "loss": 1.5191,
      "step": 300
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.5705208333333334,
      "eval_loss": 1.3643465042114258,
      "eval_runtime": 150.4083,
      "eval_samples_per_second": 63.826,
      "eval_steps_per_second": 1.995,
      "step": 300
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 7.553743839263916,
      "learning_rate": 0.0009962962962962963,
      "loss": 1.5862,
      "step": 310
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 6.20314884185791,
      "learning_rate": 0.0009925925925925927,
      "loss": 1.5179,
      "step": 320
    },
    {
      "epoch": 1.1,
      "grad_norm": 5.285332679748535,
      "learning_rate": 0.000988888888888889,
      "loss": 1.4663,
      "step": 330
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 8.19298267364502,
      "learning_rate": 0.000985185185185185,
      "loss": 1.414,
      "step": 340
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 13.449857711791992,
      "learning_rate": 0.0009814814814814816,
      "loss": 1.4157,
      "step": 350
    },
    {
      "epoch": 1.2,
      "grad_norm": 7.8103508949279785,
      "learning_rate": 0.0009777777777777777,
      "loss": 1.4225,
      "step": 360
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 7.311031818389893,
      "learning_rate": 0.0009740740740740741,
      "loss": 1.4657,
      "step": 370
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 5.933553695678711,
      "learning_rate": 0.0009703703703703704,
      "loss": 1.4014,
      "step": 380
    },
    {
      "epoch": 1.3,
      "grad_norm": 26.91130828857422,
      "learning_rate": 0.0009666666666666667,
      "loss": 1.4721,
      "step": 390
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 10.799817085266113,
      "learning_rate": 0.0009629629629629629,
      "loss": 1.4183,
      "step": 400
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 9.550079345703125,
      "learning_rate": 0.0009592592592592593,
      "loss": 1.4722,
      "step": 410
    },
    {
      "epoch": 1.4,
      "grad_norm": 5.389063835144043,
      "learning_rate": 0.0009555555555555556,
      "loss": 1.5076,
      "step": 420
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 3.8423779010772705,
      "learning_rate": 0.0009518518518518518,
      "loss": 1.4542,
      "step": 430
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 6.536604404449463,
      "learning_rate": 0.0009481481481481482,
      "loss": 1.4603,
      "step": 440
    },
    {
      "epoch": 1.5,
      "grad_norm": 5.723638534545898,
      "learning_rate": 0.0009444444444444445,
      "loss": 1.376,
      "step": 450
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 7.779978275299072,
      "learning_rate": 0.0009407407407407408,
      "loss": 1.415,
      "step": 460
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 10.049155235290527,
      "learning_rate": 0.000937037037037037,
      "loss": 1.3163,
      "step": 470
    },
    {
      "epoch": 1.6,
      "grad_norm": 7.933804035186768,
      "learning_rate": 0.0009333333333333333,
      "loss": 1.4097,
      "step": 480
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 5.085000038146973,
      "learning_rate": 0.0009296296296296296,
      "loss": 1.3244,
      "step": 490
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 4.004000186920166,
      "learning_rate": 0.000925925925925926,
      "loss": 1.3161,
      "step": 500
    },
    {
      "epoch": 1.7,
      "grad_norm": 5.95897102355957,
      "learning_rate": 0.0009222222222222223,
      "loss": 1.3049,
      "step": 510
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 5.183005332946777,
      "learning_rate": 0.0009185185185185185,
      "loss": 1.3445,
      "step": 520
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 4.614657402038574,
      "learning_rate": 0.0009148148148148149,
      "loss": 1.3704,
      "step": 530
    },
    {
      "epoch": 1.8,
      "grad_norm": 4.2919206619262695,
      "learning_rate": 0.0009111111111111111,
      "loss": 1.2667,
      "step": 540
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 5.6598052978515625,
      "learning_rate": 0.0009074074074074074,
      "loss": 1.3124,
      "step": 550
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 3.523322582244873,
      "learning_rate": 0.0009037037037037037,
      "loss": 1.3009,
      "step": 560
    },
    {
      "epoch": 1.9,
      "grad_norm": 5.342668533325195,
      "learning_rate": 0.0009000000000000001,
      "loss": 1.3175,
      "step": 570
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 4.8218278884887695,
      "learning_rate": 0.0008962962962962963,
      "loss": 1.3143,
      "step": 580
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 4.640110015869141,
      "learning_rate": 0.0008925925925925926,
      "loss": 1.2783,
      "step": 590
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.564502239227295,
      "learning_rate": 0.0008888888888888888,
      "loss": 1.3157,
      "step": 600
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6315625,
      "eval_loss": 1.1680397987365723,
      "eval_runtime": 149.1996,
      "eval_samples_per_second": 64.343,
      "eval_steps_per_second": 2.011,
      "step": 600
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 3.266885280609131,
      "learning_rate": 0.0008851851851851853,
      "loss": 1.3138,
      "step": 610
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 5.836230754852295,
      "learning_rate": 0.0008814814814814816,
      "loss": 1.3667,
      "step": 620
    },
    {
      "epoch": 2.1,
      "grad_norm": 6.686216831207275,
      "learning_rate": 0.0008777777777777778,
      "loss": 1.1763,
      "step": 630
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 6.663851737976074,
      "learning_rate": 0.0008740740740740741,
      "loss": 1.3239,
      "step": 640
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 19.34286880493164,
      "learning_rate": 0.0008703703703703704,
      "loss": 1.2691,
      "step": 650
    },
    {
      "epoch": 2.2,
      "grad_norm": 2.72489333152771,
      "learning_rate": 0.0008666666666666667,
      "loss": 1.2937,
      "step": 660
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 2.9259843826293945,
      "learning_rate": 0.0008629629629629629,
      "loss": 1.1175,
      "step": 670
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 6.132350444793701,
      "learning_rate": 0.0008592592592592593,
      "loss": 1.3646,
      "step": 680
    },
    {
      "epoch": 2.3,
      "grad_norm": 2.9719698429107666,
      "learning_rate": 0.0008555555555555556,
      "loss": 1.2298,
      "step": 690
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 9.795783996582031,
      "learning_rate": 0.0008518518518518519,
      "loss": 1.2701,
      "step": 700
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 4.2056498527526855,
      "learning_rate": 0.0008481481481481481,
      "loss": 1.2793,
      "step": 710
    },
    {
      "epoch": 2.4,
      "grad_norm": 2.4884204864501953,
      "learning_rate": 0.0008444444444444444,
      "loss": 1.1996,
      "step": 720
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 3.4484920501708984,
      "learning_rate": 0.0008407407407407409,
      "loss": 1.2005,
      "step": 730
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 3.6051061153411865,
      "learning_rate": 0.0008370370370370371,
      "loss": 1.2065,
      "step": 740
    },
    {
      "epoch": 2.5,
      "grad_norm": 8.000374794006348,
      "learning_rate": 0.0008333333333333334,
      "loss": 1.1637,
      "step": 750
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 6.4923624992370605,
      "learning_rate": 0.0008296296296296296,
      "loss": 1.2697,
      "step": 760
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 5.571068286895752,
      "learning_rate": 0.0008259259259259259,
      "loss": 1.2496,
      "step": 770
    },
    {
      "epoch": 2.6,
      "grad_norm": 4.836601257324219,
      "learning_rate": 0.0008222222222222222,
      "loss": 1.1015,
      "step": 780
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 8.633487701416016,
      "learning_rate": 0.0008185185185185186,
      "loss": 1.1967,
      "step": 790
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 3.370225191116333,
      "learning_rate": 0.0008148148148148148,
      "loss": 1.1915,
      "step": 800
    },
    {
      "epoch": 2.7,
      "grad_norm": 3.0979952812194824,
      "learning_rate": 0.0008111111111111111,
      "loss": 1.2,
      "step": 810
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 3.5940895080566406,
      "learning_rate": 0.0008074074074074075,
      "loss": 1.2024,
      "step": 820
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 5.458423137664795,
      "learning_rate": 0.0008037037037037037,
      "loss": 1.1368,
      "step": 830
    },
    {
      "epoch": 2.8,
      "grad_norm": 4.6439995765686035,
      "learning_rate": 0.0008,
      "loss": 1.1747,
      "step": 840
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 3.91074538230896,
      "learning_rate": 0.0007962962962962962,
      "loss": 1.1568,
      "step": 850
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 6.138334274291992,
      "learning_rate": 0.0007925925925925927,
      "loss": 1.1123,
      "step": 860
    },
    {
      "epoch": 2.9,
      "grad_norm": 6.5981221199035645,
      "learning_rate": 0.0007888888888888889,
      "loss": 1.2531,
      "step": 870
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 4.223883628845215,
      "learning_rate": 0.0007851851851851852,
      "loss": 1.1395,
      "step": 880
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 3.169909954071045,
      "learning_rate": 0.0007814814814814814,
      "loss": 1.138,
      "step": 890
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.6419949531555176,
      "learning_rate": 0.0007777777777777778,
      "loss": 1.1177,
      "step": 900
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.7109375,
      "eval_loss": 0.948623776435852,
      "eval_runtime": 152.5162,
      "eval_samples_per_second": 62.944,
      "eval_steps_per_second": 1.967,
      "step": 900
    },
    {
      "epoch": 3.033333333333333,
      "grad_norm": 5.404876708984375,
      "learning_rate": 0.000774074074074074,
      "loss": 1.1433,
      "step": 910
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 2.457984447479248,
      "learning_rate": 0.0007703703703703704,
      "loss": 1.0743,
      "step": 920
    },
    {
      "epoch": 3.1,
      "grad_norm": 3.517441749572754,
      "learning_rate": 0.0007666666666666667,
      "loss": 1.1202,
      "step": 930
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 4.534327030181885,
      "learning_rate": 0.000762962962962963,
      "loss": 1.1105,
      "step": 940
    },
    {
      "epoch": 3.1666666666666665,
      "grad_norm": 3.756953239440918,
      "learning_rate": 0.0007592592592592593,
      "loss": 1.0347,
      "step": 950
    },
    {
      "epoch": 3.2,
      "grad_norm": 2.31870436668396,
      "learning_rate": 0.0007555555555555555,
      "loss": 1.0391,
      "step": 960
    },
    {
      "epoch": 3.2333333333333334,
      "grad_norm": 3.954495906829834,
      "learning_rate": 0.0007518518518518519,
      "loss": 1.1629,
      "step": 970
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 3.5808374881744385,
      "learning_rate": 0.0007481481481481482,
      "loss": 1.1328,
      "step": 980
    },
    {
      "epoch": 3.3,
      "grad_norm": 6.797739028930664,
      "learning_rate": 0.0007444444444444445,
      "loss": 1.0929,
      "step": 990
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 5.666640758514404,
      "learning_rate": 0.0007407407407407407,
      "loss": 1.0837,
      "step": 1000
    },
    {
      "epoch": 3.3666666666666667,
      "grad_norm": 6.195326328277588,
      "learning_rate": 0.000737037037037037,
      "loss": 1.0658,
      "step": 1010
    },
    {
      "epoch": 3.4,
      "grad_norm": 3.572143316268921,
      "learning_rate": 0.0007333333333333333,
      "loss": 1.1564,
      "step": 1020
    },
    {
      "epoch": 3.4333333333333336,
      "grad_norm": 8.019548416137695,
      "learning_rate": 0.0007296296296296297,
      "loss": 1.083,
      "step": 1030
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 3.5930120944976807,
      "learning_rate": 0.000725925925925926,
      "loss": 1.0776,
      "step": 1040
    },
    {
      "epoch": 3.5,
      "grad_norm": 3.1695494651794434,
      "learning_rate": 0.0007222222222222222,
      "loss": 1.0998,
      "step": 1050
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 2.802079439163208,
      "learning_rate": 0.0007185185185185186,
      "loss": 1.0878,
      "step": 1060
    },
    {
      "epoch": 3.5666666666666664,
      "grad_norm": 5.332357883453369,
      "learning_rate": 0.0007148148148148148,
      "loss": 1.0596,
      "step": 1070
    },
    {
      "epoch": 3.6,
      "grad_norm": 2.3657398223876953,
      "learning_rate": 0.0007111111111111111,
      "loss": 1.0519,
      "step": 1080
    },
    {
      "epoch": 3.6333333333333333,
      "grad_norm": 3.1130547523498535,
      "learning_rate": 0.0007074074074074074,
      "loss": 1.0402,
      "step": 1090
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 3.662264347076416,
      "learning_rate": 0.0007037037037037038,
      "loss": 1.1132,
      "step": 1100
    },
    {
      "epoch": 3.7,
      "grad_norm": 5.080272674560547,
      "learning_rate": 0.0007,
      "loss": 0.9648,
      "step": 1110
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 2.3846824169158936,
      "learning_rate": 0.0006962962962962963,
      "loss": 1.0613,
      "step": 1120
    },
    {
      "epoch": 3.7666666666666666,
      "grad_norm": 3.3135344982147217,
      "learning_rate": 0.0006925925925925925,
      "loss": 1.0745,
      "step": 1130
    },
    {
      "epoch": 3.8,
      "grad_norm": 2.886491537094116,
      "learning_rate": 0.000688888888888889,
      "loss": 1.0303,
      "step": 1140
    },
    {
      "epoch": 3.8333333333333335,
      "grad_norm": 4.25195837020874,
      "learning_rate": 0.0006851851851851853,
      "loss": 1.1144,
      "step": 1150
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 3.496159315109253,
      "learning_rate": 0.0006814814814814815,
      "loss": 0.9801,
      "step": 1160
    },
    {
      "epoch": 3.9,
      "grad_norm": 2.050440788269043,
      "learning_rate": 0.0006777777777777778,
      "loss": 1.0535,
      "step": 1170
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 4.625491142272949,
      "learning_rate": 0.0006740740740740741,
      "loss": 0.995,
      "step": 1180
    },
    {
      "epoch": 3.966666666666667,
      "grad_norm": 2.4170753955841064,
      "learning_rate": 0.0006703703703703704,
      "loss": 0.9659,
      "step": 1190
    },
    {
      "epoch": 4.0,
      "grad_norm": 2.772108316421509,
      "learning_rate": 0.0006666666666666666,
      "loss": 0.9908,
      "step": 1200
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.7386458333333333,
      "eval_loss": 0.8552079200744629,
      "eval_runtime": 143.3332,
      "eval_samples_per_second": 66.977,
      "eval_steps_per_second": 2.093,
      "step": 1200
    },
    {
      "epoch": 4.033333333333333,
      "grad_norm": 4.037608623504639,
      "learning_rate": 0.000662962962962963,
      "loss": 0.996,
      "step": 1210
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 5.324876308441162,
      "learning_rate": 0.0006592592592592592,
      "loss": 0.9219,
      "step": 1220
    },
    {
      "epoch": 4.1,
      "grad_norm": 2.2522339820861816,
      "learning_rate": 0.0006555555555555556,
      "loss": 0.9768,
      "step": 1230
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 2.6154494285583496,
      "learning_rate": 0.0006518518518518519,
      "loss": 0.9419,
      "step": 1240
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 2.655989408493042,
      "learning_rate": 0.0006481481481481481,
      "loss": 0.8992,
      "step": 1250
    },
    {
      "epoch": 4.2,
      "grad_norm": 2.5698964595794678,
      "learning_rate": 0.0006444444444444444,
      "loss": 1.0083,
      "step": 1260
    },
    {
      "epoch": 4.233333333333333,
      "grad_norm": 2.125375270843506,
      "learning_rate": 0.0006407407407407408,
      "loss": 1.0297,
      "step": 1270
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 6.491453170776367,
      "learning_rate": 0.0006370370370370371,
      "loss": 0.9858,
      "step": 1280
    },
    {
      "epoch": 4.3,
      "grad_norm": 3.473013162612915,
      "learning_rate": 0.0006333333333333333,
      "loss": 0.9582,
      "step": 1290
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 3.2622604370117188,
      "learning_rate": 0.0006296296296296296,
      "loss": 1.0416,
      "step": 1300
    },
    {
      "epoch": 4.366666666666666,
      "grad_norm": 4.074906349182129,
      "learning_rate": 0.0006259259259259259,
      "loss": 0.914,
      "step": 1310
    },
    {
      "epoch": 4.4,
      "grad_norm": 2.748105764389038,
      "learning_rate": 0.0006222222222222223,
      "loss": 0.9497,
      "step": 1320
    },
    {
      "epoch": 4.433333333333334,
      "grad_norm": 4.154930114746094,
      "learning_rate": 0.0006185185185185185,
      "loss": 0.9475,
      "step": 1330
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 3.571861505508423,
      "learning_rate": 0.0006148148148148148,
      "loss": 0.9412,
      "step": 1340
    },
    {
      "epoch": 4.5,
      "grad_norm": 8.418806076049805,
      "learning_rate": 0.0006111111111111112,
      "loss": 0.9934,
      "step": 1350
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 2.5699996948242188,
      "learning_rate": 0.0006074074074074074,
      "loss": 0.9413,
      "step": 1360
    },
    {
      "epoch": 4.566666666666666,
      "grad_norm": 3.1193835735321045,
      "learning_rate": 0.0006037037037037037,
      "loss": 0.9226,
      "step": 1370
    },
    {
      "epoch": 4.6,
      "grad_norm": 2.0988314151763916,
      "learning_rate": 0.0006,
      "loss": 0.9249,
      "step": 1380
    },
    {
      "epoch": 4.633333333333333,
      "grad_norm": 3.626641035079956,
      "learning_rate": 0.0005962962962962964,
      "loss": 1.0372,
      "step": 1390
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 2.1288490295410156,
      "learning_rate": 0.0005925925925925926,
      "loss": 0.9546,
      "step": 1400
    },
    {
      "epoch": 4.7,
      "grad_norm": 2.7534637451171875,
      "learning_rate": 0.0005888888888888889,
      "loss": 0.9406,
      "step": 1410
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 2.6512372493743896,
      "learning_rate": 0.0005851851851851851,
      "loss": 0.9677,
      "step": 1420
    },
    {
      "epoch": 4.766666666666667,
      "grad_norm": 2.2737622261047363,
      "learning_rate": 0.0005814814814814815,
      "loss": 0.9008,
      "step": 1430
    },
    {
      "epoch": 4.8,
      "grad_norm": 4.1906304359436035,
      "learning_rate": 0.0005777777777777778,
      "loss": 0.865,
      "step": 1440
    },
    {
      "epoch": 4.833333333333333,
      "grad_norm": 3.6090075969696045,
      "learning_rate": 0.0005740740740740741,
      "loss": 0.9123,
      "step": 1450
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 2.344630241394043,
      "learning_rate": 0.0005703703703703704,
      "loss": 0.9245,
      "step": 1460
    },
    {
      "epoch": 4.9,
      "grad_norm": 3.2023775577545166,
      "learning_rate": 0.0005666666666666667,
      "loss": 0.9393,
      "step": 1470
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 4.090478420257568,
      "learning_rate": 0.000562962962962963,
      "loss": 0.9481,
      "step": 1480
    },
    {
      "epoch": 4.966666666666667,
      "grad_norm": 2.54972505569458,
      "learning_rate": 0.0005592592592592592,
      "loss": 0.9383,
      "step": 1490
    },
    {
      "epoch": 5.0,
      "grad_norm": 5.06699800491333,
      "learning_rate": 0.0005555555555555556,
      "loss": 0.9341,
      "step": 1500
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.759375,
      "eval_loss": 0.7888408899307251,
      "eval_runtime": 155.5506,
      "eval_samples_per_second": 61.716,
      "eval_steps_per_second": 1.929,
      "step": 1500
    },
    {
      "epoch": 5.033333333333333,
      "grad_norm": 3.005202293395996,
      "learning_rate": 0.0005518518518518519,
      "loss": 0.9064,
      "step": 1510
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 5.14912748336792,
      "learning_rate": 0.0005481481481481482,
      "loss": 0.8171,
      "step": 1520
    },
    {
      "epoch": 5.1,
      "grad_norm": 1.8327924013137817,
      "learning_rate": 0.0005444444444444444,
      "loss": 0.8717,
      "step": 1530
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 2.006610870361328,
      "learning_rate": 0.0005407407407407407,
      "loss": 0.8337,
      "step": 1540
    },
    {
      "epoch": 5.166666666666667,
      "grad_norm": 2.9984705448150635,
      "learning_rate": 0.0005370370370370371,
      "loss": 0.9009,
      "step": 1550
    },
    {
      "epoch": 5.2,
      "grad_norm": 2.820173978805542,
      "learning_rate": 0.0005333333333333334,
      "loss": 0.8016,
      "step": 1560
    },
    {
      "epoch": 5.233333333333333,
      "grad_norm": 2.0023839473724365,
      "learning_rate": 0.0005296296296296297,
      "loss": 0.8661,
      "step": 1570
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 2.7798118591308594,
      "learning_rate": 0.0005259259259259259,
      "loss": 0.8892,
      "step": 1580
    },
    {
      "epoch": 5.3,
      "grad_norm": 2.6698713302612305,
      "learning_rate": 0.0005222222222222223,
      "loss": 0.847,
      "step": 1590
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 4.081544399261475,
      "learning_rate": 0.0005185185185185185,
      "loss": 0.9223,
      "step": 1600
    },
    {
      "epoch": 5.366666666666666,
      "grad_norm": 4.435242652893066,
      "learning_rate": 0.0005148148148148148,
      "loss": 0.9132,
      "step": 1610
    },
    {
      "epoch": 5.4,
      "grad_norm": 4.008460998535156,
      "learning_rate": 0.0005111111111111111,
      "loss": 0.8631,
      "step": 1620
    },
    {
      "epoch": 5.433333333333334,
      "grad_norm": 2.0855090618133545,
      "learning_rate": 0.0005074074074074075,
      "loss": 0.7916,
      "step": 1630
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 1.818316102027893,
      "learning_rate": 0.0005037037037037037,
      "loss": 0.9207,
      "step": 1640
    },
    {
      "epoch": 5.5,
      "grad_norm": 2.5171937942504883,
      "learning_rate": 0.0005,
      "loss": 0.8391,
      "step": 1650
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 3.7515981197357178,
      "learning_rate": 0.0004962962962962963,
      "loss": 0.9427,
      "step": 1660
    },
    {
      "epoch": 5.566666666666666,
      "grad_norm": 3.2098307609558105,
      "learning_rate": 0.0004925925925925925,
      "loss": 0.8979,
      "step": 1670
    },
    {
      "epoch": 5.6,
      "grad_norm": 2.1433753967285156,
      "learning_rate": 0.0004888888888888889,
      "loss": 0.8227,
      "step": 1680
    },
    {
      "epoch": 5.633333333333333,
      "grad_norm": 2.4450223445892334,
      "learning_rate": 0.0004851851851851852,
      "loss": 0.882,
      "step": 1690
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 1.8940800428390503,
      "learning_rate": 0.00048148148148148144,
      "loss": 0.8662,
      "step": 1700
    },
    {
      "epoch": 5.7,
      "grad_norm": 2.418442726135254,
      "learning_rate": 0.0004777777777777778,
      "loss": 0.8644,
      "step": 1710
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 1.8941649198532104,
      "learning_rate": 0.0004740740740740741,
      "loss": 0.8429,
      "step": 1720
    },
    {
      "epoch": 5.766666666666667,
      "grad_norm": 2.124709129333496,
      "learning_rate": 0.0004703703703703704,
      "loss": 0.841,
      "step": 1730
    },
    {
      "epoch": 5.8,
      "grad_norm": 2.2716503143310547,
      "learning_rate": 0.00046666666666666666,
      "loss": 0.8135,
      "step": 1740
    },
    {
      "epoch": 5.833333333333333,
      "grad_norm": 2.0669779777526855,
      "learning_rate": 0.000462962962962963,
      "loss": 0.8422,
      "step": 1750
    },
    {
      "epoch": 5.866666666666667,
      "grad_norm": 2.5293736457824707,
      "learning_rate": 0.00045925925925925925,
      "loss": 0.8701,
      "step": 1760
    },
    {
      "epoch": 5.9,
      "grad_norm": 2.690281867980957,
      "learning_rate": 0.00045555555555555556,
      "loss": 0.8637,
      "step": 1770
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 3.43405818939209,
      "learning_rate": 0.00045185185185185183,
      "loss": 0.8286,
      "step": 1780
    },
    {
      "epoch": 5.966666666666667,
      "grad_norm": 1.9131993055343628,
      "learning_rate": 0.00044814814814814815,
      "loss": 0.8812,
      "step": 1790
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.9487468004226685,
      "learning_rate": 0.0004444444444444444,
      "loss": 0.8016,
      "step": 1800
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.7815625,
      "eval_loss": 0.7242977619171143,
      "eval_runtime": 142.9876,
      "eval_samples_per_second": 67.139,
      "eval_steps_per_second": 2.098,
      "step": 1800
    },
    {
      "epoch": 6.033333333333333,
      "grad_norm": 2.2791059017181396,
      "learning_rate": 0.0004407407407407408,
      "loss": 0.8219,
      "step": 1810
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 8.489875793457031,
      "learning_rate": 0.00043703703703703705,
      "loss": 0.7623,
      "step": 1820
    },
    {
      "epoch": 6.1,
      "grad_norm": 3.6432266235351562,
      "learning_rate": 0.00043333333333333337,
      "loss": 0.7757,
      "step": 1830
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 3.050814151763916,
      "learning_rate": 0.00042962962962962963,
      "loss": 0.7984,
      "step": 1840
    },
    {
      "epoch": 6.166666666666667,
      "grad_norm": 2.2111051082611084,
      "learning_rate": 0.00042592592592592595,
      "loss": 0.7469,
      "step": 1850
    },
    {
      "epoch": 6.2,
      "grad_norm": 2.828071117401123,
      "learning_rate": 0.0004222222222222222,
      "loss": 0.7078,
      "step": 1860
    },
    {
      "epoch": 6.233333333333333,
      "grad_norm": 1.7600489854812622,
      "learning_rate": 0.00041851851851851853,
      "loss": 0.7752,
      "step": 1870
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 3.4225878715515137,
      "learning_rate": 0.0004148148148148148,
      "loss": 0.7846,
      "step": 1880
    },
    {
      "epoch": 6.3,
      "grad_norm": 3.5138680934906006,
      "learning_rate": 0.0004111111111111111,
      "loss": 0.6758,
      "step": 1890
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 2.998642683029175,
      "learning_rate": 0.0004074074074074074,
      "loss": 0.7931,
      "step": 1900
    },
    {
      "epoch": 6.366666666666666,
      "grad_norm": 9.792279243469238,
      "learning_rate": 0.00040370370370370375,
      "loss": 0.791,
      "step": 1910
    },
    {
      "epoch": 6.4,
      "grad_norm": 2.541327714920044,
      "learning_rate": 0.0004,
      "loss": 0.7366,
      "step": 1920
    },
    {
      "epoch": 6.433333333333334,
      "grad_norm": 1.421732783317566,
      "learning_rate": 0.00039629629629629634,
      "loss": 0.7403,
      "step": 1930
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 2.2584071159362793,
      "learning_rate": 0.0003925925925925926,
      "loss": 0.7705,
      "step": 1940
    },
    {
      "epoch": 6.5,
      "grad_norm": 5.697581768035889,
      "learning_rate": 0.0003888888888888889,
      "loss": 0.746,
      "step": 1950
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 3.4076147079467773,
      "learning_rate": 0.0003851851851851852,
      "loss": 0.6949,
      "step": 1960
    },
    {
      "epoch": 6.566666666666666,
      "grad_norm": 3.2590973377227783,
      "learning_rate": 0.0003814814814814815,
      "loss": 0.79,
      "step": 1970
    },
    {
      "epoch": 6.6,
      "grad_norm": 2.997842788696289,
      "learning_rate": 0.00037777777777777777,
      "loss": 0.7891,
      "step": 1980
    },
    {
      "epoch": 6.633333333333333,
      "grad_norm": 2.0552585124969482,
      "learning_rate": 0.0003740740740740741,
      "loss": 0.7515,
      "step": 1990
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 2.335519790649414,
      "learning_rate": 0.00037037037037037035,
      "loss": 0.7015,
      "step": 2000
    },
    {
      "epoch": 6.7,
      "grad_norm": 2.5474531650543213,
      "learning_rate": 0.00036666666666666667,
      "loss": 0.8021,
      "step": 2010
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 3.159796714782715,
      "learning_rate": 0.000362962962962963,
      "loss": 0.8085,
      "step": 2020
    },
    {
      "epoch": 6.766666666666667,
      "grad_norm": 2.377361297607422,
      "learning_rate": 0.0003592592592592593,
      "loss": 0.7114,
      "step": 2030
    },
    {
      "epoch": 6.8,
      "grad_norm": 4.104302883148193,
      "learning_rate": 0.00035555555555555557,
      "loss": 0.7824,
      "step": 2040
    },
    {
      "epoch": 6.833333333333333,
      "grad_norm": 1.6216199398040771,
      "learning_rate": 0.0003518518518518519,
      "loss": 0.7696,
      "step": 2050
    },
    {
      "epoch": 6.866666666666667,
      "grad_norm": 5.111682891845703,
      "learning_rate": 0.00034814814814814816,
      "loss": 0.7435,
      "step": 2060
    },
    {
      "epoch": 6.9,
      "grad_norm": 1.993964433670044,
      "learning_rate": 0.0003444444444444445,
      "loss": 0.762,
      "step": 2070
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 2.8675012588500977,
      "learning_rate": 0.00034074074074074074,
      "loss": 0.6868,
      "step": 2080
    },
    {
      "epoch": 6.966666666666667,
      "grad_norm": 7.776313781738281,
      "learning_rate": 0.00033703703703703706,
      "loss": 0.7968,
      "step": 2090
    },
    {
      "epoch": 7.0,
      "grad_norm": 3.716637372970581,
      "learning_rate": 0.0003333333333333333,
      "loss": 0.7109,
      "step": 2100
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.7951041666666666,
      "eval_loss": 0.6738744378089905,
      "eval_runtime": 144.5253,
      "eval_samples_per_second": 66.424,
      "eval_steps_per_second": 2.076,
      "step": 2100
    },
    {
      "epoch": 7.033333333333333,
      "grad_norm": 1.7939155101776123,
      "learning_rate": 0.0003296296296296296,
      "loss": 0.7345,
      "step": 2110
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 1.6092661619186401,
      "learning_rate": 0.00032592592592592596,
      "loss": 0.6684,
      "step": 2120
    },
    {
      "epoch": 7.1,
      "grad_norm": 2.141116142272949,
      "learning_rate": 0.0003222222222222222,
      "loss": 0.6748,
      "step": 2130
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 3.317095994949341,
      "learning_rate": 0.00031851851851851854,
      "loss": 0.7132,
      "step": 2140
    },
    {
      "epoch": 7.166666666666667,
      "grad_norm": 4.767422199249268,
      "learning_rate": 0.0003148148148148148,
      "loss": 0.6733,
      "step": 2150
    },
    {
      "epoch": 7.2,
      "grad_norm": 2.6786491870880127,
      "learning_rate": 0.0003111111111111111,
      "loss": 0.7147,
      "step": 2160
    },
    {
      "epoch": 7.233333333333333,
      "grad_norm": 4.159494400024414,
      "learning_rate": 0.0003074074074074074,
      "loss": 0.7389,
      "step": 2170
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 1.8425425291061401,
      "learning_rate": 0.0003037037037037037,
      "loss": 0.7089,
      "step": 2180
    },
    {
      "epoch": 7.3,
      "grad_norm": 2.630547523498535,
      "learning_rate": 0.0003,
      "loss": 0.7034,
      "step": 2190
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 1.7583503723144531,
      "learning_rate": 0.0002962962962962963,
      "loss": 0.6833,
      "step": 2200
    },
    {
      "epoch": 7.366666666666666,
      "grad_norm": 2.5891880989074707,
      "learning_rate": 0.00029259259259259256,
      "loss": 0.7422,
      "step": 2210
    },
    {
      "epoch": 7.4,
      "grad_norm": 4.601663112640381,
      "learning_rate": 0.0002888888888888889,
      "loss": 0.7069,
      "step": 2220
    },
    {
      "epoch": 7.433333333333334,
      "grad_norm": 2.448239803314209,
      "learning_rate": 0.0002851851851851852,
      "loss": 0.6733,
      "step": 2230
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 1.8270460367202759,
      "learning_rate": 0.0002814814814814815,
      "loss": 0.6087,
      "step": 2240
    },
    {
      "epoch": 7.5,
      "grad_norm": 2.6170668601989746,
      "learning_rate": 0.0002777777777777778,
      "loss": 0.6448,
      "step": 2250
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 1.9415346384048462,
      "learning_rate": 0.0002740740740740741,
      "loss": 0.6145,
      "step": 2260
    },
    {
      "epoch": 7.566666666666666,
      "grad_norm": 2.730590581893921,
      "learning_rate": 0.00027037037037037036,
      "loss": 0.6856,
      "step": 2270
    },
    {
      "epoch": 7.6,
      "grad_norm": 1.8843389749526978,
      "learning_rate": 0.0002666666666666667,
      "loss": 0.6764,
      "step": 2280
    },
    {
      "epoch": 7.633333333333333,
      "grad_norm": 1.9028739929199219,
      "learning_rate": 0.00026296296296296294,
      "loss": 0.71,
      "step": 2290
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 3.220060110092163,
      "learning_rate": 0.00025925925925925926,
      "loss": 0.7159,
      "step": 2300
    },
    {
      "epoch": 7.7,
      "grad_norm": 1.4293670654296875,
      "learning_rate": 0.00025555555555555553,
      "loss": 0.6585,
      "step": 2310
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 2.037872076034546,
      "learning_rate": 0.00025185185185185185,
      "loss": 0.6702,
      "step": 2320
    },
    {
      "epoch": 7.766666666666667,
      "grad_norm": 1.7540905475616455,
      "learning_rate": 0.00024814814814814816,
      "loss": 0.6263,
      "step": 2330
    },
    {
      "epoch": 7.8,
      "grad_norm": 2.566361665725708,
      "learning_rate": 0.00024444444444444443,
      "loss": 0.6693,
      "step": 2340
    },
    {
      "epoch": 7.833333333333333,
      "grad_norm": 2.0912675857543945,
      "learning_rate": 0.00024074074074074072,
      "loss": 0.6419,
      "step": 2350
    },
    {
      "epoch": 7.866666666666667,
      "grad_norm": 2.725241184234619,
      "learning_rate": 0.00023703703703703704,
      "loss": 0.6402,
      "step": 2360
    },
    {
      "epoch": 7.9,
      "grad_norm": 9.654829978942871,
      "learning_rate": 0.00023333333333333333,
      "loss": 0.6596,
      "step": 2370
    },
    {
      "epoch": 7.933333333333334,
      "grad_norm": 1.608784794807434,
      "learning_rate": 0.00022962962962962962,
      "loss": 0.674,
      "step": 2380
    },
    {
      "epoch": 7.966666666666667,
      "grad_norm": 1.7957628965377808,
      "learning_rate": 0.00022592592592592591,
      "loss": 0.6304,
      "step": 2390
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.7356460094451904,
      "learning_rate": 0.0002222222222222222,
      "loss": 0.6324,
      "step": 2400
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.8004166666666667,
      "eval_loss": 0.6550253033638,
      "eval_runtime": 144.4375,
      "eval_samples_per_second": 66.465,
      "eval_steps_per_second": 2.077,
      "step": 2400
    },
    {
      "epoch": 8.033333333333333,
      "grad_norm": 2.5819451808929443,
      "learning_rate": 0.00021851851851851852,
      "loss": 0.6157,
      "step": 2410
    },
    {
      "epoch": 8.066666666666666,
      "grad_norm": 2.2930264472961426,
      "learning_rate": 0.00021481481481481482,
      "loss": 0.6668,
      "step": 2420
    },
    {
      "epoch": 8.1,
      "grad_norm": 3.3987739086151123,
      "learning_rate": 0.0002111111111111111,
      "loss": 0.5883,
      "step": 2430
    },
    {
      "epoch": 8.133333333333333,
      "grad_norm": 2.8157832622528076,
      "learning_rate": 0.0002074074074074074,
      "loss": 0.5942,
      "step": 2440
    },
    {
      "epoch": 8.166666666666666,
      "grad_norm": 3.5139222145080566,
      "learning_rate": 0.0002037037037037037,
      "loss": 0.5932,
      "step": 2450
    },
    {
      "epoch": 8.2,
      "grad_norm": 2.5507187843322754,
      "learning_rate": 0.0002,
      "loss": 0.6214,
      "step": 2460
    },
    {
      "epoch": 8.233333333333333,
      "grad_norm": 2.7017605304718018,
      "learning_rate": 0.0001962962962962963,
      "loss": 0.5905,
      "step": 2470
    },
    {
      "epoch": 8.266666666666667,
      "grad_norm": 2.708794116973877,
      "learning_rate": 0.0001925925925925926,
      "loss": 0.564,
      "step": 2480
    },
    {
      "epoch": 8.3,
      "grad_norm": 1.995316982269287,
      "learning_rate": 0.00018888888888888888,
      "loss": 0.5976,
      "step": 2490
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 3.895106077194214,
      "learning_rate": 0.00018518518518518518,
      "loss": 0.5938,
      "step": 2500
    },
    {
      "epoch": 8.366666666666667,
      "grad_norm": 2.22593092918396,
      "learning_rate": 0.0001814814814814815,
      "loss": 0.5849,
      "step": 2510
    },
    {
      "epoch": 8.4,
      "grad_norm": 2.7342095375061035,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.5423,
      "step": 2520
    },
    {
      "epoch": 8.433333333333334,
      "grad_norm": 2.0175883769989014,
      "learning_rate": 0.00017407407407407408,
      "loss": 0.568,
      "step": 2530
    },
    {
      "epoch": 8.466666666666667,
      "grad_norm": 2.0456838607788086,
      "learning_rate": 0.00017037037037037037,
      "loss": 0.5349,
      "step": 2540
    },
    {
      "epoch": 8.5,
      "grad_norm": 1.620066523551941,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.538,
      "step": 2550
    },
    {
      "epoch": 8.533333333333333,
      "grad_norm": 2.7286007404327393,
      "learning_rate": 0.00016296296296296298,
      "loss": 0.5482,
      "step": 2560
    },
    {
      "epoch": 8.566666666666666,
      "grad_norm": 3.4174888134002686,
      "learning_rate": 0.00015925925925925927,
      "loss": 0.6184,
      "step": 2570
    },
    {
      "epoch": 8.6,
      "grad_norm": 4.093010425567627,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.5476,
      "step": 2580
    },
    {
      "epoch": 8.633333333333333,
      "grad_norm": 2.1469149589538574,
      "learning_rate": 0.00015185185185185185,
      "loss": 0.5779,
      "step": 2590
    },
    {
      "epoch": 8.666666666666666,
      "grad_norm": 2.3034894466400146,
      "learning_rate": 0.00014814814814814815,
      "loss": 0.5521,
      "step": 2600
    },
    {
      "epoch": 8.7,
      "grad_norm": 5.304210662841797,
      "learning_rate": 0.00014444444444444444,
      "loss": 0.5111,
      "step": 2610
    },
    {
      "epoch": 8.733333333333333,
      "grad_norm": 2.14923357963562,
      "learning_rate": 0.00014074074074074076,
      "loss": 0.5604,
      "step": 2620
    },
    {
      "epoch": 8.766666666666667,
      "grad_norm": 2.760037422180176,
      "learning_rate": 0.00013703703703703705,
      "loss": 0.5788,
      "step": 2630
    },
    {
      "epoch": 8.8,
      "grad_norm": 1.8928877115249634,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.6152,
      "step": 2640
    },
    {
      "epoch": 8.833333333333334,
      "grad_norm": 2.463019371032715,
      "learning_rate": 0.00012962962962962963,
      "loss": 0.5194,
      "step": 2650
    },
    {
      "epoch": 8.866666666666667,
      "grad_norm": 2.1824190616607666,
      "learning_rate": 0.00012592592592592592,
      "loss": 0.5749,
      "step": 2660
    },
    {
      "epoch": 8.9,
      "grad_norm": 2.127011299133301,
      "learning_rate": 0.00012222222222222221,
      "loss": 0.5729,
      "step": 2670
    },
    {
      "epoch": 8.933333333333334,
      "grad_norm": 1.7865984439849854,
      "learning_rate": 0.00011851851851851852,
      "loss": 0.5535,
      "step": 2680
    },
    {
      "epoch": 8.966666666666667,
      "grad_norm": 1.756347417831421,
      "learning_rate": 0.00011481481481481481,
      "loss": 0.5118,
      "step": 2690
    },
    {
      "epoch": 9.0,
      "grad_norm": 2.6204607486724854,
      "learning_rate": 0.0001111111111111111,
      "loss": 0.577,
      "step": 2700
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.8230208333333333,
      "eval_loss": 0.6179793477058411,
      "eval_runtime": 141.0873,
      "eval_samples_per_second": 68.043,
      "eval_steps_per_second": 2.126,
      "step": 2700
    },
    {
      "epoch": 9.033333333333333,
      "grad_norm": 3.641068458557129,
      "learning_rate": 0.00010740740740740741,
      "loss": 0.4456,
      "step": 2710
    },
    {
      "epoch": 9.066666666666666,
      "grad_norm": 1.9577085971832275,
      "learning_rate": 0.0001037037037037037,
      "loss": 0.525,
      "step": 2720
    },
    {
      "epoch": 9.1,
      "grad_norm": 2.765411853790283,
      "learning_rate": 0.0001,
      "loss": 0.473,
      "step": 2730
    },
    {
      "epoch": 9.133333333333333,
      "grad_norm": 1.8569902181625366,
      "learning_rate": 9.62962962962963e-05,
      "loss": 0.4956,
      "step": 2740
    },
    {
      "epoch": 9.166666666666666,
      "grad_norm": 1.6194462776184082,
      "learning_rate": 9.259259259259259e-05,
      "loss": 0.4884,
      "step": 2750
    },
    {
      "epoch": 9.2,
      "grad_norm": 2.1341726779937744,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.5311,
      "step": 2760
    },
    {
      "epoch": 9.233333333333333,
      "grad_norm": 2.1057753562927246,
      "learning_rate": 8.518518518518518e-05,
      "loss": 0.4933,
      "step": 2770
    },
    {
      "epoch": 9.266666666666667,
      "grad_norm": 2.215730905532837,
      "learning_rate": 8.148148148148149e-05,
      "loss": 0.4694,
      "step": 2780
    },
    {
      "epoch": 9.3,
      "grad_norm": 4.229648113250732,
      "learning_rate": 7.777777777777778e-05,
      "loss": 0.4998,
      "step": 2790
    },
    {
      "epoch": 9.333333333333334,
      "grad_norm": 5.050601482391357,
      "learning_rate": 7.407407407407407e-05,
      "loss": 0.4577,
      "step": 2800
    },
    {
      "epoch": 9.366666666666667,
      "grad_norm": 1.993634581565857,
      "learning_rate": 7.037037037037038e-05,
      "loss": 0.5072,
      "step": 2810
    },
    {
      "epoch": 9.4,
      "grad_norm": 4.2038350105285645,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.5119,
      "step": 2820
    },
    {
      "epoch": 9.433333333333334,
      "grad_norm": 2.6873762607574463,
      "learning_rate": 6.296296296296296e-05,
      "loss": 0.506,
      "step": 2830
    },
    {
      "epoch": 9.466666666666667,
      "grad_norm": 3.9159998893737793,
      "learning_rate": 5.925925925925926e-05,
      "loss": 0.4606,
      "step": 2840
    },
    {
      "epoch": 9.5,
      "grad_norm": 1.5484716892242432,
      "learning_rate": 5.555555555555555e-05,
      "loss": 0.459,
      "step": 2850
    },
    {
      "epoch": 9.533333333333333,
      "grad_norm": 3.3061373233795166,
      "learning_rate": 5.185185185185185e-05,
      "loss": 0.453,
      "step": 2860
    },
    {
      "epoch": 9.566666666666666,
      "grad_norm": 4.106355667114258,
      "learning_rate": 4.814814814814815e-05,
      "loss": 0.5618,
      "step": 2870
    },
    {
      "epoch": 9.6,
      "grad_norm": 2.5664682388305664,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.4876,
      "step": 2880
    },
    {
      "epoch": 9.633333333333333,
      "grad_norm": 2.017333745956421,
      "learning_rate": 4.0740740740740745e-05,
      "loss": 0.4932,
      "step": 2890
    },
    {
      "epoch": 9.666666666666666,
      "grad_norm": 1.9828799962997437,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 0.4924,
      "step": 2900
    },
    {
      "epoch": 9.7,
      "grad_norm": 1.6560121774673462,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.4722,
      "step": 2910
    },
    {
      "epoch": 9.733333333333333,
      "grad_norm": 1.3015989065170288,
      "learning_rate": 2.962962962962963e-05,
      "loss": 0.4374,
      "step": 2920
    },
    {
      "epoch": 9.766666666666667,
      "grad_norm": 3.8949177265167236,
      "learning_rate": 2.5925925925925925e-05,
      "loss": 0.4968,
      "step": 2930
    },
    {
      "epoch": 9.8,
      "grad_norm": 1.7859622240066528,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.464,
      "step": 2940
    },
    {
      "epoch": 9.833333333333334,
      "grad_norm": 3.031079053878784,
      "learning_rate": 1.8518518518518518e-05,
      "loss": 0.4895,
      "step": 2950
    },
    {
      "epoch": 9.866666666666667,
      "grad_norm": 4.9201765060424805,
      "learning_rate": 1.4814814814814815e-05,
      "loss": 0.4402,
      "step": 2960
    },
    {
      "epoch": 9.9,
      "grad_norm": 2.1636905670166016,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 0.4555,
      "step": 2970
    },
    {
      "epoch": 9.933333333333334,
      "grad_norm": 1.5664026737213135,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 0.467,
      "step": 2980
    },
    {
      "epoch": 9.966666666666667,
      "grad_norm": 3.9578068256378174,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 0.4823,
      "step": 2990
    },
    {
      "epoch": 10.0,
      "grad_norm": 1.7420895099639893,
      "learning_rate": 0.0,
      "loss": 0.4771,
      "step": 3000
    },
    {
      "epoch": 10.0,
      "eval_accuracy": 0.8270833333333333,
      "eval_loss": 0.593846321105957,
      "eval_runtime": 142.6658,
      "eval_samples_per_second": 67.29,
      "eval_steps_per_second": 2.103,
      "step": 3000
    },
    {
      "epoch": 10.0,
      "step": 3000,
      "total_flos": 9.547438968637194e+18,
      "train_loss": 0.9688654476801555,
      "train_runtime": 10935.8074,
      "train_samples_per_second": 35.11,
      "train_steps_per_second": 0.274
    },
    {
      "epoch": 10.0,
      "eval_accuracy": 0.8270833333333333,
      "eval_loss": 0.593846321105957,
      "eval_runtime": 142.6039,
      "eval_samples_per_second": 67.319,
      "eval_steps_per_second": 2.104,
      "step": 3000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.547438968637194e+18,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
